{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads oohca dataset info and processes it to find relevant cuts. Placed in `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import scipy\n",
    "\n",
    "from cci.utils import project_dir\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATASET_FOLDER = project_dir() / \"data\"\n",
    "DATASET_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "# NOTE: Change this to directory containing the `.mat` files\n",
    "OOCHA_DIR = Path(os.environ[\"OOCHA_DIR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load info\n",
    "arecs = scipy.io.loadmat(OOCHA_DIR / \"arecs.mat\", simplify_cells=True)[\"arecs\"]\n",
    "oohca_info = scipy.io.loadmat(OOCHA_DIR / \"oohrepr.mat\", simplify_cells=True)[\"oohrepr\"]\n",
    "\n",
    "\n",
    "def replace_object(x):\n",
    "    return [[y] if isinstance(y, str) else y.tolist() for y in x]\n",
    "\n",
    "\n",
    "for k, v in oohca_info.items():\n",
    "    oohca_info[k] = replace_object(v)\n",
    "oohca_info.update({\"file\": arecs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in dataframe, change annotations and save a copy of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values\n",
    "df = (\n",
    "    pl.LazyFrame(\n",
    "        {key: oohca_info[key] for key in [\"file\", \"EPI\", \"SMP\"]},\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"file\").is_not_null(),  # Some of the entries are missing filename\n",
    "        # Remove rows where these are different (rows with 1 rythm)\n",
    "        pl.col(\"EPI\").list.eval(pl.element().len()) == pl.col(\"SMP\").list.eval(pl.element().len()),\n",
    "    )\n",
    "    .explode(\"EPI\", \"SMP\")\n",
    "    .with_columns(\n",
    "        # Extract start/stop\n",
    "        pl.col(\"SMP\").list.to_struct(\n",
    "            fields=[\"Start\", \"Stop\"],\n",
    "        ),\n",
    "    )\n",
    "    .unnest(\"SMP\")\n",
    ")\n",
    "\n",
    "\n",
    "# Annotate\n",
    "def annotate_hands_off(epi: str) -> str:\n",
    "    \"\"\"Hands off AS -> HAS\"\"\"\n",
    "    mappings = {\n",
    "        \"AS\": \"HAS\",\n",
    "        \"pr\": \"hpr\",\n",
    "        \"VF\": \"HVF\",\n",
    "        \"PR\": \"HPR\",\n",
    "        \"as\": \"has\",\n",
    "        \"vf\": \"hvf\",\n",
    "        \"pe\": \"hpe\",\n",
    "        \"VT\": \"HVT\",\n",
    "        \"PE\": \"HPE\",\n",
    "        \"vt\": \"hvt\",\n",
    "        \"un\": \"hun\",\n",
    "    }\n",
    "    return mappings.get(epi, epi)\n",
    "\n",
    "\n",
    "def map_dfb(vals: pl.Struct) -> str:\n",
    "    \"\"\"VT -> dfb -> VT => VT -> DVT -> VT\"\"\"\n",
    "    prev = vals[\"epi_-1\"]\n",
    "    current = vals[\"epi_0\"]\n",
    "    next = vals[\"epi_1\"]\n",
    "    if current == \"dfb\" and prev == next:\n",
    "        return f\"D{prev}\"\n",
    "    else:\n",
    "        return current\n",
    "\n",
    "\n",
    "# Annotate 'dfb' with corresponding rythm\n",
    "\n",
    "df = (\n",
    "    df.with_columns(\n",
    "        [pl.col(\"EPI\").shift(-i).alias(f\"epi_{i}\") for i in range(-1, 2)],\n",
    "    )\n",
    "    .with_columns(pl.struct([\"epi_-1\", \"epi_0\", \"epi_1\"]).map_elements(map_dfb).alias(\"EPI\"))\n",
    "    .with_columns(pl.col(\"EPI\").map_elements(annotate_hands_off))\n",
    "    # BUG\n",
    "    # .drop(\n",
    "    #     [f\"epi_{i}\" for i in range(-1, 2)],\n",
    "    # )\n",
    ")\n",
    "\n",
    "# Collect and save\n",
    "# BUG:?? Have to drop here or Start and Stop also gets dropped....\n",
    "df = df.collect().drop(\n",
    "    [f\"epi_{i}\" for i in range(-1, 2)],\n",
    ")\n",
    "df.write_csv(DATASET_FOLDER / \"full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFB Dataset\n",
    "Collect relevant DFB cuts and save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfb_df(full_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Filter so we have rhytm -> D(fb)rythm -> rythm -> transition\n",
    "    df = (\n",
    "        full_df.with_columns(\n",
    "            [pl.col(\"EPI\").shift(-i).alias(f\"epi{i}\") for i in range(1, 4)],\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"Start\").shift(-i).alias(f\"start{i}\") for i in range(1, 4)],\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"Stop\").shift(-i).alias(f\"stop{i}\") for i in range(1, 4)],\n",
    "        )\n",
    "        .filter(pl.col(\"epi1\").str.starts_with(\"D\"))\n",
    "    )\n",
    "    # Filter when occuring at end of file\n",
    "    df = df.filter(pl.min_horizontal(pl.col(\"start1\"), pl.col(\"start2\"), pl.col(\"start3\")) != 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dfb_full = dfb_df(df)\n",
    "dfb_full.write_csv(DATASET_FOLDER / \"dfb_full.csv\")\n",
    "\n",
    "dfb = dfb_full.clone()\n",
    "# Filter when last is D(fb) or C(ompression) (no transition) TODO: Or should these be bad transitions?\n",
    "dfb = dfb.filter(~pl.col(\"epi3\").str.starts_with(\"D\"))\n",
    "dfb = dfb.filter(~pl.col(\"epi3\").str.starts_with(\"C\"))\n",
    "\n",
    "# Filter out unknowns\n",
    "dfb = dfb.filter(~pl.col(\"EPI\").str.contains(\"un\"))\n",
    "\n",
    "# Keep only if < 3 seconds after dfb there is a transition\n",
    "dfb = dfb.filter((pl.col(\"start3\") - pl.col(\"start2\")) < 1500).with_row_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using verify_dataset to visually verify correct transitions\n",
    "Shift transitions that are incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of signals overlapping with dfb\n",
    "\n",
    "shift = [\n",
    "    9,\n",
    "    12,\n",
    "    14,\n",
    "    16,\n",
    "    22,\n",
    "    28,\n",
    "    29,\n",
    "    34,\n",
    "    37,\n",
    "    41,\n",
    "    46,\n",
    "    47,\n",
    "    52,\n",
    "    55,\n",
    "    59,\n",
    "    62,\n",
    "    70,\n",
    "    71,\n",
    "    72,\n",
    "    99,\n",
    "    101,\n",
    "    103,\n",
    "    110,\n",
    "    112,\n",
    "    116,\n",
    "    117,\n",
    "    125,\n",
    "    126,\n",
    "    133,\n",
    "    142,\n",
    "    143,\n",
    "    147,\n",
    "    148,\n",
    "    164,\n",
    "    168,\n",
    "    169,\n",
    "    170,\n",
    "    171,\n",
    "    172,\n",
    "    173,\n",
    "    176,\n",
    "    177,\n",
    "    178,\n",
    "    180,\n",
    "    181,\n",
    "    182,\n",
    "    183,\n",
    "    193,\n",
    "    200,\n",
    "    201,\n",
    "    202,\n",
    "    203,\n",
    "    204,\n",
    "    210,\n",
    "    212,\n",
    "    214,\n",
    "    216,\n",
    "    224,\n",
    "    233,\n",
    "    237,\n",
    "    240,\n",
    "    241,\n",
    "    244,\n",
    "    247,\n",
    "    252,\n",
    "    255,\n",
    "    265,\n",
    "    278,\n",
    "    281,\n",
    "    291,\n",
    "    294,\n",
    "    296,\n",
    "    303,\n",
    "    308,\n",
    "    313,\n",
    "    315,\n",
    "    322,\n",
    "    323,\n",
    "    324,\n",
    "    329,\n",
    "    332,\n",
    "    334,\n",
    "    335,\n",
    "    338,\n",
    "    341,\n",
    "    345,\n",
    "    349,\n",
    "    351,\n",
    "    352,\n",
    "    354,\n",
    "    366,\n",
    "    369,\n",
    "    383,\n",
    "    386,\n",
    "    387,\n",
    "    390,\n",
    "    392,\n",
    "    393,\n",
    "    398,\n",
    "    405,\n",
    "    412,\n",
    "    414,\n",
    "    416,\n",
    "    421,\n",
    "    427,\n",
    "    428,\n",
    "    430,\n",
    "    431,\n",
    "    435,\n",
    "    436,\n",
    "    437,\n",
    "    449,\n",
    "    451,\n",
    "    453,\n",
    "    454,\n",
    "    456,\n",
    "    459,\n",
    "    460,\n",
    "    462,\n",
    "    464,\n",
    "    465,\n",
    "    467,\n",
    "    475,\n",
    "    487,\n",
    "    491,\n",
    "    497,\n",
    "    499,\n",
    "    500,\n",
    "]\n",
    "\n",
    "# --- Shift late transitions before dfb\n",
    "dfb_overrides_shift = dfb.filter(pl.col(\"index\").is_in(shift))\n",
    "# Shift transition by 0.01 seconds\n",
    "shift_samples = int(500 * 0.01)\n",
    "dfb_overrides_shift = dfb_overrides_shift.with_columns(\n",
    "    pl.col(\"Stop\") - shift_samples,\n",
    "    pl.col(\"start1\") - shift_samples,\n",
    "    pl.lit(\"shift_transition\").alias(\"override_function\"),\n",
    "    pl.lit(\"overlap with DFB\").alias(\"override_reason\"),\n",
    "    pl.lit(True).alias(\"valid\"),\n",
    ")\n",
    "\n",
    "# --- Fix other errors\n",
    "errors = [\n",
    "    (105, \"start\", 10, \"contains unannotated shock\"),\n",
    "    (194, \"start\", 3, \"starts with unannotated shock\"),\n",
    "    (283, \"remove\", None, \"hvf, only sample annotated as noisy\"),\n",
    "    (419, \"remove\", None, \"HAS, should be CAS?\"),\n",
    "]\n",
    "\n",
    "dfb_overrides_errors = dfb.filter(pl.col(\"index\").is_in([x[0] for x in errors]))\n",
    "\n",
    "\n",
    "def adjust_start(row):\n",
    "    if row[\"index\"] == 105:\n",
    "        return row[\"Start\"] + 10 * 500\n",
    "    elif row[\"index\"] == 194:\n",
    "        return row[\"Start\"] + 3 * 500\n",
    "    return row[\"Start\"]\n",
    "\n",
    "\n",
    "dfb_overrides_errors = dfb_overrides_errors.with_columns(\n",
    "    pl.struct([\"index\", \"Start\"]).map_elements(adjust_start).alias(\"Start\"),\n",
    "    override_function=np.array([\"shift_start\", \"shift_start\", \"remove\", \"remove\"]),\n",
    "    override_reason=np.array([\"contains unannotated shock\", \"contains unannotated shock\", \"noisy\", \"noisy\"]),\n",
    "    valid=np.array([True, True, False, False]),\n",
    ")\n",
    "\n",
    "dfb_overrides = dfb_overrides_shift.vstack(dfb_overrides_errors).sort(\"index\")\n",
    "\n",
    "dfb_overrides.write_csv(DATASET_FOLDER / \"dfb_overrides.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the revised dfb set to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with overrides\n",
    "dfb.filter(~pl.col(\"index\").is_in(dfb_overrides.select(\"index\"))).vstack(\n",
    "    dfb_overrides.drop([\"override_function\", \"override_reason\", \"valid\"])\n",
    ").filter(  # Remove unvalid indexes\n",
    "    ~pl.col(\"index\").is_in(dfb_overrides.filter(~pl.col(\"valid\")).select(\"index\")),\n",
    ").sort(\"index\").write_csv(DATASET_FOLDER / \"dfb.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean transition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(DATASET_FOLDER / \"full.csv\")\n",
    "\n",
    "\n",
    "def dfb_clean(full_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = (\n",
    "        # Keep unnoisy hands off recordings\n",
    "        full_df.with_columns(\n",
    "            [pl.col(\"EPI\").shift(-i).alias(f\"epi{i}\") for i in range(1, 2)],\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"Start\").shift(-i).alias(f\"start{i}\") for i in range(1, 2)],\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"Stop\").shift(-i).alias(f\"stop{i}\") for i in range(1, 2)],\n",
    "        )\n",
    "        .filter(\n",
    "            # Keep only Clean Signal -> Clean Signal (hands off, not noisy)\n",
    "            pl.col(\"EPI\").str.starts_with(\"H\"),\n",
    "            pl.col(\"epi1\").str.starts_with(\"H\"),\n",
    "            # Remove if last\n",
    "            pl.col(\"start1\") != 1,\n",
    "        )\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "clean_df_full = dfb_clean(df).filter((pl.col(\"Stop\") - pl.col(\"Start\")) > 1500)\n",
    "clean_df_full.write_csv(DATASET_FOLDER / \"clean_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override clean df (only checked when length > 3 seconds)\n",
    "override_df = clean_df_full.with_columns(\n",
    "    pl.lit(True).alias(\"Valid\"),\n",
    "    pl.lit(None).alias(\"New Start\"),\n",
    "    pl.lit(None).alias(\"New Stop\"),\n",
    "    pl.lit(None).alias(\"comment\"),\n",
    ").with_row_index()\n",
    "override_df.write_csv(DATASET_FOLDER / \"override_df_template.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix override clean after check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (589, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>index</th><th>file</th><th>EPI</th><th>Start</th><th>Stop</th><th>epi1</th><th>start1</th><th>stop1</th><th>Valid</th><th>New Start</th><th>New Stop</th><th>comment</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>i64</td><td>i64</td><td>str</td><td>i64</td><td>i64</td><td>bool</td><td>str</td><td>i64</td><td>str</td></tr></thead><tbody><tr><td>1</td><td>&quot;a_2&quot;</td><td>&quot;HPE&quot;</td><td>457313</td><td>466699</td><td>&quot;HVT&quot;</td><td>466700</td><td>470691</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>&quot;a_2&quot;</td><td>&quot;HPE&quot;</td><td>826803</td><td>829500</td><td>&quot;HPR&quot;</td><td>829501</td><td>936774</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>&quot;a_2&quot;</td><td>&quot;HPR&quot;</td><td>1114462</td><td>1121524</td><td>&quot;HPE&quot;</td><td>1121525</td><td>1141336</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>4</td><td>&quot;a_2&quot;</td><td>&quot;HPE&quot;</td><td>1121525</td><td>1141336</td><td>&quot;HVT&quot;</td><td>1141337</td><td>1148379</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>5</td><td>&quot;a_2&quot;</td><td>&quot;HVT&quot;</td><td>1141337</td><td>1148379</td><td>&quot;HVF&quot;</td><td>1148380</td><td>1190708</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>655</td><td>&quot;s_383&quot;</td><td>&quot;HVF&quot;</td><td>162427</td><td>171755</td><td>&quot;HAS&quot;</td><td>171756</td><td>176902</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>656</td><td>&quot;s_383&quot;</td><td>&quot;HPE&quot;</td><td>200564</td><td>205199</td><td>&quot;HAS&quot;</td><td>205200</td><td>218862</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>657</td><td>&quot;s_383&quot;</td><td>&quot;HPE&quot;</td><td>262358</td><td>265438</td><td>&quot;HAS&quot;</td><td>265439</td><td>267223</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>658</td><td>&quot;s_386&quot;</td><td>&quot;HPE&quot;</td><td>418190</td><td>419747</td><td>&quot;HAS&quot;</td><td>419748</td><td>428167</td><td>true</td><td>null</td><td>null</td><td>null</td></tr><tr><td>661</td><td>&quot;s_393&quot;</td><td>&quot;HAS&quot;</td><td>5564</td><td>9774</td><td>&quot;HVF&quot;</td><td>10209</td><td>14441</td><td>true</td><td>null</td><td>9774</td><td>&quot;Move transitio…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (589, 12)\n",
       "┌───────┬───────┬─────┬─────────┬───┬───────┬───────────┬──────────┬─────────────────┐\n",
       "│ index ┆ file  ┆ EPI ┆ Start   ┆ … ┆ Valid ┆ New Start ┆ New Stop ┆ comment         │\n",
       "│ ---   ┆ ---   ┆ --- ┆ ---     ┆   ┆ ---   ┆ ---       ┆ ---      ┆ ---             │\n",
       "│ i64   ┆ str   ┆ str ┆ i64     ┆   ┆ bool  ┆ str       ┆ i64      ┆ str             │\n",
       "╞═══════╪═══════╪═════╪═════════╪═══╪═══════╪═══════════╪══════════╪═════════════════╡\n",
       "│ 1     ┆ a_2   ┆ HPE ┆ 457313  ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 2     ┆ a_2   ┆ HPE ┆ 826803  ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 3     ┆ a_2   ┆ HPR ┆ 1114462 ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 4     ┆ a_2   ┆ HPE ┆ 1121525 ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 5     ┆ a_2   ┆ HVT ┆ 1141337 ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ …     ┆ …     ┆ …   ┆ …       ┆ … ┆ …     ┆ …         ┆ …        ┆ …               │\n",
       "│ 655   ┆ s_383 ┆ HVF ┆ 162427  ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 656   ┆ s_383 ┆ HPE ┆ 200564  ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 657   ┆ s_383 ┆ HPE ┆ 262358  ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 658   ┆ s_386 ┆ HPE ┆ 418190  ┆ … ┆ true  ┆ null      ┆ null     ┆ null            │\n",
       "│ 661   ┆ s_393 ┆ HAS ┆ 5564    ┆ … ┆ true  ┆ null      ┆ 9774     ┆ Move transition │\n",
       "└───────┴───────┴─────┴─────────┴───┴───────┴───────────┴──────────┴─────────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pl.read_csv(DATASET_FOLDER / \"override_df.csv\")\n",
    "clean_df = clean_df.filter(pl.col(\"Valid\"))\n",
    "\n",
    "\n",
    "clean_df.with_columns(\n",
    "    (\n",
    "        pl.when(pl.col(\"New Stop\").is_not_null())\n",
    "        .then(\n",
    "            pl.col(\"New Stop\"),\n",
    "        )\n",
    "        .otherwise(\n",
    "            pl.col(\"Stop\"),\n",
    "        )\n",
    "    )\n",
    "    .cast(int)\n",
    "    .alias(\"Stop\"),\n",
    "    (\n",
    "        pl.when(pl.col(\"New Start\").is_not_null())\n",
    "        .then(\n",
    "            pl.col(\"New Start\"),\n",
    "        )\n",
    "        .otherwise(\n",
    "            pl.col(\"Start\"),\n",
    "        )\n",
    "    )\n",
    "    .cast(int)\n",
    "    .alias(\"Start\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training/test/validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify clean set\n",
    "\n",
    "\n",
    "def classify_class_label(df):\n",
    "    \"\"\"Class 0 good, class 1 bad\n",
    "    next_epi: 1 for EPI_1, 2 for EPI_2\n",
    "    Desired:\n",
    "    VF/VT -> PR\n",
    "    AS -> PR / VF / VT\n",
    "    PE -> PR\n",
    "    PR -> sROSC TODO:\n",
    "    \"\"\"\n",
    "    class_label = []\n",
    "    for epi, next_epi in df.select([\"EPI\", \"EPI_NEXT\"]).rows():\n",
    "        match epi:\n",
    "            case \"HAS\":\n",
    "                if next_epi in [\"HVF\", \"HVT\", \"HPR\"]:\n",
    "                    class_label.append(0)\n",
    "                else:\n",
    "                    class_label.append(1)\n",
    "            case \"HVF\":\n",
    "                if next_epi in [\"HPR\"]:\n",
    "                    class_label.append(0)\n",
    "                else:\n",
    "                    class_label.append(1)\n",
    "            case \"HVT\":\n",
    "                if next_epi in [\"HPR\"]:\n",
    "                    class_label.append(0)\n",
    "                else:\n",
    "                    class_label.append(1)\n",
    "            case \"HPE\":\n",
    "                if next_epi in [\"HPR\"]:\n",
    "                    class_label.append(0)\n",
    "                else:\n",
    "                    class_label.append(1)\n",
    "            case \"HPR\":\n",
    "                class_label.append(1)\n",
    "    return class_label\n",
    "\n",
    "\n",
    "clean_df = (\n",
    "    clean_df.filter((pl.col(\"Stop\") - pl.col(\"Start\")) > 1500)\n",
    "    .select([\"file\", \"Start\", \"Stop\", \"EPI\", \"epi1\"])\n",
    "    .rename({\"epi1\": \"EPI_NEXT\"})\n",
    ")\n",
    "dfb_df = (\n",
    "    pl.read_csv(DATASET_FOLDER / \"dfb.csv\")\n",
    "    .filter((pl.col(\"Stop\") - pl.col(\"Start\")) > 1500)\n",
    "    .select([\"file\", \"Start\", \"Stop\", \"EPI\", \"epi3\"])\n",
    "    .rename({\"epi3\": \"EPI_NEXT\"})\n",
    ")\n",
    "# Minimum 3 seconds\n",
    "\n",
    "df = clean_df.vstack(dfb_df)\n",
    "class_label = classify_class_label(df)\n",
    "df = df.hstack([pl.Series(\"Class\", class_label)]).sort(\"file\", \"Start\")\n",
    "df.write_csv(DATASET_FOLDER / \"test_train_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
    "\n",
    "RANDOM_STATE = 0\n",
    "df = pl.read_csv(DATASET_FOLDER / \"test_train_val.csv\").with_row_index()\n",
    "# Split train/test\n",
    "labels = df.select(\"Class\").to_series().to_numpy()\n",
    "train_val_idx, test_idx = train_test_split(\n",
    "    range(len(df)),\n",
    "    stratify=labels,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "train_val_df = df.filter(pl.col(\"index\").is_in(train_val_idx))\n",
    "test_df = df.filter(pl.col(\"index\").is_in(test_idx))\n",
    "\n",
    "# Reindex\n",
    "train_val_df = train_val_df.drop(\"index\").with_row_index()\n",
    "test_df = test_df.drop(\"index\").with_row_index()\n",
    "test_df.write_csv(DATASET_FOLDER / \"full_test.csv\")\n",
    "\n",
    "# Slit train/validation\n",
    "labels = train_val_df.select(\"Class\").to_series().to_numpy()\n",
    "groups = train_val_df.select(\"file\").to_series().to_numpy()\n",
    "fold = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "for i, (train_idx, val_idx) in enumerate(fold.split(np.zeros(len(train_val_df)), labels, groups)):\n",
    "    train_val_df.filter(pl.col(\"index\").is_in(train_idx)).write_csv(DATASET_FOLDER / f\"full_train_{i}.csv\")\n",
    "    train_val_df.filter(pl.col(\"index\").is_in(val_idx)).write_csv(DATASET_FOLDER / f\"full_val_{i}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training sets for each rhythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine HVF/HVT\n",
    "vft_df = df.filter(pl.col(\"EPI\").is_in([\"HVF\", \"HVT\"]))\n",
    "as_df = df.filter(pl.col(\"EPI\") == \"HAS\")\n",
    "pe_df = df.filter(pl.col(\"EPI\") == \"HPE\")\n",
    "pr_df = df.filter(pl.col(\"EPI\") == \"HPR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = vft_df.select(\"Class\").to_series().to_numpy()\n",
    "train_idx, test_idx = train_test_split(\n",
    "    range(len(vft_df)),\n",
    "    stratify=labels,\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "vft_df = vft_df.drop(\"index\").with_row_index()\n",
    "train = vft_df.filter(pl.col(\"index\").is_in(train_idx))\n",
    "test = vft_df.filter(pl.col(\"index\").is_in(test_idx))\n",
    "train.write_csv(DATASET_FOLDER / \"vft_train.csv\")\n",
    "test.write_csv(DATASET_FOLDER / \"vft_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rhytm_name, rhytm_df in [\n",
    "    (\"vft\", vft_df),\n",
    "    (\"as\", as_df),\n",
    "    (\"pe\", pe_df),\n",
    "    (\"pr\", pr_df),\n",
    "]:\n",
    "    labels = rhytm_df.select(\"Class\").to_series().to_numpy()\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        range(len(rhytm_df)),\n",
    "        stratify=labels,\n",
    "        test_size=0.1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    rhytm_df = rhytm_df.drop(\"index\").with_row_index()\n",
    "    train = rhytm_df.filter(pl.col(\"index\").is_in(train_idx))\n",
    "    test = rhytm_df.filter(pl.col(\"index\").is_in(test_idx))\n",
    "    train.write_csv(DATASET_FOLDER / f\"{rhytm_name}_train_0.csv\")\n",
    "    test.write_csv(DATASET_FOLDER / f\"{rhytm_name}_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
