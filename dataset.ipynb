{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loads oohca dataset info and processes it to find relevant cuts. Placed in `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "import scipy\n",
    "\n",
    "from cci.utils import project_dir\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DATASET_FOLDER = project_dir() / \"data\"\n",
    "DATASET_FOLDER.mkdir(exist_ok=True)\n",
    "OOCHA_DIR = Path(os.environ[\"OOCHA_DIR\"])\n",
    "\n",
    "# Load info\n",
    "arecs = scipy.io.loadmat(OOCHA_DIR / \"arecs.mat\", simplify_cells=True)[\"arecs\"]\n",
    "oohca_info = scipy.io.loadmat(OOCHA_DIR / \"oohrepr.mat\", simplify_cells=True)[\"oohrepr\"]\n",
    "\n",
    "\n",
    "def replace_object(x):\n",
    "    return [[y] if isinstance(y, str) else y.tolist() for y in x]\n",
    "\n",
    "\n",
    "for k, v in oohca_info.items():\n",
    "    oohca_info[k] = replace_object(v)\n",
    "oohca_info.update({\"file\": arecs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store in dataframe, change annotations and save a copy of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values\n",
    "df = (\n",
    "    pl.LazyFrame(\n",
    "        {key: oohca_info[key] for key in [\"file\", \"EPI\", \"SMP\"]},\n",
    "    )\n",
    "    .filter(\n",
    "        pl.col(\"file\").is_not_null(),  # Some of the entries are missing filename\n",
    "        # Remove rows where these are different (rows with 1 rythm)\n",
    "        pl.col(\"EPI\").list.eval(pl.element().len()) == pl.col(\"SMP\").list.eval(pl.element().len()),\n",
    "    )\n",
    "    .explode(\"EPI\", \"SMP\")\n",
    "    .with_columns(\n",
    "        # Extract start/stop\n",
    "        pl.col(\"SMP\").list.to_struct(\n",
    "            fields=[\"Start\", \"Stop\"],\n",
    "        ),\n",
    "    )\n",
    "    .unnest(\"SMP\")\n",
    ")\n",
    "\n",
    "\n",
    "# Annotate\n",
    "def annotate_hands_off(epi: str) -> str:\n",
    "    \"\"\"Hands off AS -> HAS\"\"\"\n",
    "    mappings = {\n",
    "        \"AS\": \"HAS\",\n",
    "        \"pr\": \"hpr\",\n",
    "        \"VF\": \"HVF\",\n",
    "        \"PR\": \"HPR\",\n",
    "        \"as\": \"has\",\n",
    "        \"vf\": \"hvf\",\n",
    "        \"pe\": \"hpe\",\n",
    "        \"VT\": \"HVT\",\n",
    "        \"PE\": \"HPE\",\n",
    "        \"vt\": \"hvt\",\n",
    "        \"un\": \"hun\",\n",
    "    }\n",
    "    return mappings.get(epi, epi)\n",
    "\n",
    "\n",
    "def map_dfb(vals: pl.Struct) -> str:\n",
    "    \"\"\"VT -> dfb -> VT => VT -> DVT -> VT\"\"\"\n",
    "    prev = vals[\"epi_-1\"]\n",
    "    current = vals[\"epi_0\"]\n",
    "    next = vals[\"epi_1\"]\n",
    "    if current == \"dfb\" and prev == next:\n",
    "        return f\"D{prev}\"\n",
    "    else:\n",
    "        return current\n",
    "\n",
    "\n",
    "# Annotate 'dfb' with corresponding rythm\n",
    "\n",
    "df = (\n",
    "    df.with_columns(\n",
    "        [pl.col(\"EPI\").shift(-i).alias(f\"epi_{i}\") for i in range(-1, 2)],\n",
    "    )\n",
    "    .with_columns(pl.struct([\"epi_-1\", \"epi_0\", \"epi_1\"]).map_elements(map_dfb).alias(\"EPI\"))\n",
    "    .with_columns(pl.col(\"EPI\").map_elements(annotate_hands_off))\n",
    "    # BUG\n",
    "    # .drop(\n",
    "    #     [f\"epi_{i}\" for i in range(-1, 2)],\n",
    "    # )\n",
    ")\n",
    "\n",
    "# Collect and save\n",
    "# BUG:?? Have to drop here or Start and Stop also gets dropped....\n",
    "df = df.collect().drop(\n",
    "    [f\"epi_{i}\" for i in range(-1, 2)],\n",
    ")\n",
    "df.write_csv(DATASET_FOLDER / \"full.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect relevant DFB cuts and save as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfb_df(full_df: pl.DataFrame) -> pl.DataFrame:\n",
    "    # Filter so we have rhytm -> D(fb)rythm -> rythm -> transition\n",
    "    df = (\n",
    "        full_df.with_columns(\n",
    "            [pl.col(\"EPI\").shift(-i).alias(f\"epi{i}\") for i in range(1, 4)],\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"Start\").shift(-i).alias(f\"start{i}\") for i in range(1, 4)],\n",
    "        )\n",
    "        .with_columns(\n",
    "            [pl.col(\"Stop\").shift(-i).alias(f\"stop{i}\") for i in range(1, 4)],\n",
    "        )\n",
    "        .filter(pl.col(\"epi1\").str.starts_with(\"D\"))\n",
    "    )\n",
    "    # Filter when occuring at end of file\n",
    "    df = df.filter(pl.min_horizontal(pl.col(\"start1\"), pl.col(\"start2\"), pl.col(\"start3\")) != 1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "dfb_full = dfb_df(df)\n",
    "dfb_full.write_csv(DATASET_FOLDER / \"dfb_full.csv\")\n",
    "\n",
    "dfb = dfb_full.clone()\n",
    "# Filter when last is D(fb) or C(ompression) (no transition) TODO: Or should these be bad transitions?\n",
    "dfb = dfb.filter(~pl.col(\"epi3\").str.starts_with(\"D\"))\n",
    "dfb = dfb.filter(~pl.col(\"epi3\").str.starts_with(\"C\"))\n",
    "\n",
    "# Filter out unknowns\n",
    "dfb = dfb.filter(~pl.col(\"EPI\").str.contains(\"un\"))\n",
    "\n",
    "# Keep only if < 3 seconds after dfb there is a transition\n",
    "dfb = dfb.filter((pl.col(\"start3\") - pl.col(\"start2\")) < 1500).with_row_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using verify_dataset to visually verify correct transitions\n",
    "Shift transitions that are incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of signals overlapping with dfb\n",
    "\n",
    "shift = [\n",
    "    9,\n",
    "    12,\n",
    "    14,\n",
    "    16,\n",
    "    22,\n",
    "    28,\n",
    "    29,\n",
    "    34,\n",
    "    37,\n",
    "    41,\n",
    "    46,\n",
    "    47,\n",
    "    52,\n",
    "    55,\n",
    "    59,\n",
    "    62,\n",
    "    70,\n",
    "    71,\n",
    "    72,\n",
    "    99,\n",
    "    101,\n",
    "    103,\n",
    "    110,\n",
    "    112,\n",
    "    116,\n",
    "    117,\n",
    "    125,\n",
    "    126,\n",
    "    133,\n",
    "    142,\n",
    "    143,\n",
    "    147,\n",
    "    148,\n",
    "    164,\n",
    "    168,\n",
    "    169,\n",
    "    170,\n",
    "    171,\n",
    "    172,\n",
    "    173,\n",
    "    176,\n",
    "    177,\n",
    "    178,\n",
    "    180,\n",
    "    181,\n",
    "    182,\n",
    "    183,\n",
    "    193,\n",
    "    200,\n",
    "    201,\n",
    "    202,\n",
    "    203,\n",
    "    204,\n",
    "    210,\n",
    "    212,\n",
    "    214,\n",
    "    216,\n",
    "    224,\n",
    "    233,\n",
    "    237,\n",
    "    240,\n",
    "    241,\n",
    "    244,\n",
    "    247,\n",
    "    252,\n",
    "    255,\n",
    "    265,\n",
    "    278,\n",
    "    281,\n",
    "    291,\n",
    "    294,\n",
    "    296,\n",
    "    303,\n",
    "    308,\n",
    "    313,\n",
    "    315,\n",
    "    322,\n",
    "    323,\n",
    "    324,\n",
    "    329,\n",
    "    332,\n",
    "    334,\n",
    "    335,\n",
    "    338,\n",
    "    341,\n",
    "    345,\n",
    "    349,\n",
    "    351,\n",
    "    352,\n",
    "    354,\n",
    "    366,\n",
    "    369,\n",
    "    383,\n",
    "    386,\n",
    "    387,\n",
    "    390,\n",
    "    392,\n",
    "    393,\n",
    "    398,\n",
    "    405,\n",
    "    412,\n",
    "    414,\n",
    "    416,\n",
    "    421,\n",
    "    427,\n",
    "    428,\n",
    "    430,\n",
    "    431,\n",
    "    435,\n",
    "    436,\n",
    "    437,\n",
    "    449,\n",
    "    451,\n",
    "    453,\n",
    "    454,\n",
    "    456,\n",
    "    459,\n",
    "    460,\n",
    "    462,\n",
    "    464,\n",
    "    465,\n",
    "    467,\n",
    "    475,\n",
    "    487,\n",
    "    491,\n",
    "    497,\n",
    "    499,\n",
    "    500,\n",
    "]\n",
    "\n",
    "# --- Shift late transitions before dfb\n",
    "dfb_overrides_shift = dfb.filter(pl.col(\"index\").is_in(shift))\n",
    "# Shift transition by 0.01 seconds\n",
    "shift_samples = int(500 * 0.01)\n",
    "dfb_overrides_shift = dfb_overrides_shift.with_columns(\n",
    "    pl.col(\"Stop\") - shift_samples,\n",
    "    pl.col(\"start1\") - shift_samples,\n",
    "    pl.lit(\"shift_transition\").alias(\"override_function\"),\n",
    "    pl.lit(\"overlap with DFB\").alias(\"override_reason\"),\n",
    "    pl.lit(True).alias(\"valid\"),\n",
    ")\n",
    "\n",
    "# --- Fix other errors\n",
    "errors = [\n",
    "    (105, \"start\", 10, \"contains unannotated shock\"),\n",
    "    (194, \"start\", 3, \"starts with unannotated shock\"),\n",
    "    (283, \"remove\", None, \"hvf, only sample annotated as noisy\"),\n",
    "    (419, \"remove\", None, \"HAS, should be CAS?\"),\n",
    "]\n",
    "\n",
    "dfb_overrides_errors = dfb.filter(pl.col(\"index\").is_in([x[0] for x in errors]))\n",
    "\n",
    "\n",
    "def adjust_start(row):\n",
    "    if row[\"index\"] == 105:\n",
    "        return row[\"Start\"] + 10 * 500\n",
    "    elif row[\"index\"] == 194:\n",
    "        return row[\"Start\"] + 3 * 500\n",
    "    return row[\"Start\"]\n",
    "\n",
    "\n",
    "dfb_overrides_errors = dfb_overrides_errors.with_columns(\n",
    "    pl.struct([\"index\", \"Start\"]).map_elements(adjust_start).alias(\"Start\"),\n",
    "    override_function=np.array([\"shift_start\", \"shift_start\", \"remove\", \"remove\"]),\n",
    "    override_reason=np.array([\"contains unannotated shock\", \"contains unannotated shock\", \"noisy\", \"noisy\"]),\n",
    "    valid=np.array([True, True, False, False]),\n",
    ")\n",
    "\n",
    "dfb_overrides = dfb_overrides_shift.vstack(dfb_overrides_errors).sort(\"index\")\n",
    "\n",
    "dfb_overrides.write_csv(DATASET_FOLDER / \"dfb_overrides.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the revised dfb set to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with overrides\n",
    "dfb.filter(~pl.col(\"index\").is_in(dfb_overrides.select(\"index\"))).vstack(\n",
    "    dfb_overrides.drop([\"override_function\", \"override_reason\", \"valid\"])\n",
    ").filter(  # Remove unvalid indexes\n",
    "    ~pl.col(\"index\").is_in(dfb_overrides.filter(~pl.col(\"valid\")).select(\"index\")),\n",
    ").write_csv(DATASET_FOLDER / \"dfb.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
