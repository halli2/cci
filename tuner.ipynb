{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ROOT_DIR = Path(os.environ[\"OOCHA_DIR\"])\n",
    "RESULTS_DIR = Path(\"results\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Running with device: {torch.cuda.get_device_name(DEVICE)}\")\n",
    "\n",
    "\n",
    "def split_train_test(csv: str) -> tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    \"\"\"Returns train_val_df and test_df. Uses a fixed seed to always get the same test set\"\"\"\n",
    "    clean_df = pl.read_csv(csv).with_row_index()\n",
    "    (labels,) = clean_df.select(\"Class Label\")\n",
    "    labels = labels.to_numpy()\n",
    "\n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        range(len(clean_df)),\n",
    "        stratify=labels,\n",
    "        test_size=0.1,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    train_val_df = clean_df.filter(pl.col(\"index\").is_in(train_val_idx))\n",
    "    test_df = clean_df.filter(pl.col(\"index\").is_in(test_idx))\n",
    "\n",
    "    # Reindex\n",
    "    train_val_df = train_val_df.drop(\"index\").with_row_index()\n",
    "    test_df = test_df.drop(\"index\").with_row_index()\n",
    "    return train_val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from torch import nn\n",
    "\n",
    "from src.cci.models import LambdaModule\n",
    "\n",
    "\n",
    "def suggest_mlp(trial: optuna.Trial) -> list[nn.Module]:\n",
    "    n_hidden_layers = trial.suggest_int(\"n_hidden_layers\", 1, 3)\n",
    "    i = 1\n",
    "    layers: list[nn.Module] = [\n",
    "        nn.Flatten(),\n",
    "        nn.Dropout(trial.suggest_float(f\"dropout_{i}\", 0.1, 0.5)),\n",
    "    ]\n",
    "    features = trial.suggest_int(f\"linear_{i}\", 1, 1000)\n",
    "    prev_features = features\n",
    "    layers.append(nn.LazyLinear(features))\n",
    "    layers.append(nn.ReLU())\n",
    "    for _ in range(n_hidden_layers):\n",
    "        i += 1\n",
    "        features = trial.suggest_int(f\"linear_{i}\", 1, 1000)\n",
    "\n",
    "        layers.append(nn.Dropout(trial.suggest_float(f\"dropout_{i}\", 0.1, 0.5)))\n",
    "        layers.append(nn.Linear(prev_features, features))\n",
    "        layers.append(nn.ReLU())\n",
    "        prev_features = features\n",
    "    i += 1\n",
    "    layers.append(nn.Dropout(trial.suggest_float(f\"dropout_{i}\", 0.1, 0.5)))\n",
    "    layers.append(nn.Linear(prev_features, 1))\n",
    "    layers.append(LambdaLayer(lambda x: x.view(-1)))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "from rich.live import Live\n",
    "from rich.progress import (\n",
    "    Progress,\n",
    "    TextColumn,\n",
    "    TimeElapsedColumn,\n",
    ")\n",
    "from rich.table import Table\n",
    "from torch.nn import functional as F\n",
    "from src.cci.metrics import Metrics\n",
    "\n",
    "\n",
    "def fit(\n",
    "    model: nn.Module,\n",
    "    opt: torch.optim.Optimizer,\n",
    "    loss_fn: nn.BCEWithLogitsLoss,\n",
    "    val_loss_fn: nn.BCEWithLogitsLoss,\n",
    "    train_metrics: Metrics,\n",
    "    val_metrics: Metrics,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    epochs: int,\n",
    ") -> tuple[Metrics, Metrics, Dict[str, Any]]:\n",
    "    table = Table(\"Training model: TODO\")\n",
    "    metric_info = Progress(TextColumn(\"{task.description}\"))\n",
    "    task_metrics = metric_info.add_task(\"Metrics\")\n",
    "    progress = Progress(*Progress.get_default_columns(), TimeElapsedColumn())\n",
    "    task_epoch = progress.add_task(\"Epochs\")\n",
    "    task_train = progress.add_task(\"Train\")\n",
    "    task_validation = progress.add_task(\"Validation\")\n",
    "    table.add_row(progress)\n",
    "    table.add_row(metric_info)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_model = model.state_dict()\n",
    "    with Live(table):\n",
    "        for epoch in progress.track(range(1, epochs + 1), description=\"Epochs\", task_id=task_epoch):\n",
    "            progress.reset(task_validation)\n",
    "            train_metrics.reset()\n",
    "            model.train()\n",
    "\n",
    "            for data in progress.track(train_loader, description=\"Training\", task_id=task_train):\n",
    "                sample, label = data[\"signal\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "                opt.zero_grad()\n",
    "                logits = model(sample)\n",
    "\n",
    "                loss = loss_fn(logits, label.float())\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                predictions = F.sigmoid(logits)\n",
    "                train_metrics.update(predictions, label, loss)\n",
    "            train_metrics.save_metrics(epoch)\n",
    "\n",
    "            # Track weights\n",
    "            # track_params_dists(model, run)\n",
    "            # track_gradients_dists(model, run)\n",
    "\n",
    "            val_metrics.reset()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for data in progress.track(val_loader, description=\"Validation\", task_id=task_validation):\n",
    "                    sample, label = data[\"signal\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "                    logits = model(sample)\n",
    "\n",
    "                    loss = val_loss_fn(logits, label.float())\n",
    "\n",
    "                    predictions = F.sigmoid(logits)\n",
    "                    val_metrics.update(predictions, label, loss)\n",
    "            val_metrics.save_metrics(epoch)\n",
    "\n",
    "            validation_values = val_metrics.compute()\n",
    "            training_values = train_metrics.compute()\n",
    "            metric_info.update(\n",
    "                task_id=task_metrics,\n",
    "                description=f\"\\nTraining\\n Acc:{training_values['acc']:.3f}\\n Loss{training_values['loss']:.3f}\\n\"\n",
    "                f\"Validation\\n Acc:{validation_values['acc']:.3f}\\n Loss:{validation_values['loss']:.3f}\\n\",\n",
    "            )\n",
    "            if validation_values[\"loss\"] < best_loss:\n",
    "                best_loss = validation_values[\"loss\"]\n",
    "                best_model = model.state_dict()\n",
    "\n",
    "    return train_metrics, val_metrics, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich.progress import track\n",
    "\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    optimizer_name = trial.suggest_categorical(\"Optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16, 32])\n",
    "    model_layers = suggest_mlp(trial)\n",
    "\n",
    "    run = {\n",
    "        \"experiment\": STUDY_NAME,\n",
    "        \"dataset\": {\n",
    "            \"samples\": 1500,\n",
    "            \"preprocessing\": {},\n",
    "            \"set\": \"data/clean_df.csv\",\n",
    "            \"test_set\": {\"augmentation\": \"random_shift\"},\n",
    "        },\n",
    "        \"hparams\": {\n",
    "            \"learning_rate\": 0.002,\n",
    "            \"batch_size\": 32,\n",
    "        },\n",
    "    }\n",
    "    run[\"params\"] = trial.params\n",
    "    run_hash = DeepHash(run)[run]\n",
    "\n",
    "    # Split train/test\n",
    "    clean_df = pl.read_csv(\"data/clean_df.csv\").with_row_index()\n",
    "    (labels,) = clean_df.select(\"Class Label\")\n",
    "    labels = labels.to_numpy()\n",
    "\n",
    "    train_val_idx, test_idx = train_test_split(\n",
    "        range(len(clean_df)),\n",
    "        stratify=labels,\n",
    "        test_size=0.1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    train_val_df = clean_df.filter(pl.col(\"index\").is_in(train_val_idx))\n",
    "    test_df = clean_df.filter(pl.col(\"index\").is_in(test_idx))\n",
    "    # Reindex\n",
    "    train_val_df = train_val_df.drop(\"index\").with_row_index()\n",
    "    test_df = test_df.drop(\"index\").with_row_index()\n",
    "\n",
    "    running_bac = 0.0\n",
    "    running_loss = 0.0\n",
    "    running_f1 = 0.0\n",
    "    splits = 5\n",
    "    fold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=0)\n",
    "    for fold_idx, (train_idx, val_idx) in enumerate(fold.split(np.zeros(len(train_val_df)), labels[train_val_idx])):\n",
    "        model = nn.Sequential(*model_layers).to(DEVICE)\n",
    "        if fold_idx == 0:\n",
    "            run[\"model_arch\"] = str(model)\n",
    "            run[\"dir\"] = RESULTS_DIR / run[\"experiment\"] / run_hash\n",
    "            run[\"dir\"].mkdir(parents=True, exist_ok=True)\n",
    "            trial.user_attrs[\"dir\"] = run[\"dir\"]\n",
    "        fold_dir: Path = run[\"dir\"] / str(fold_idx)\n",
    "        fold_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        train_df = train_val_df.filter(pl.col(\"index\").is_in(train_idx))\n",
    "        val_df = train_val_df.filter(pl.col(\"index\").is_in(val_idx))\n",
    "        root_dir = Path(os.environ[\"OOCHA_DIR\"])\n",
    "        train_dataset = TransitionDataset(\n",
    "            train_df,\n",
    "            ROOT_DIR,\n",
    "            transforms=[\n",
    "                RandomSample(SAMPLE_LENGTH),\n",
    "                ToTensor(),\n",
    "            ],\n",
    "        )\n",
    "        val_dataset = TransitionDataset(\n",
    "            val_df,\n",
    "            ROOT_DIR,\n",
    "            transforms=[\n",
    "                CropSample(SAMPLE_LENGTH),\n",
    "                ToTensor(),\n",
    "            ],\n",
    "        )\n",
    "        test_dataset = TransitionDataset(\n",
    "            test_df,\n",
    "            ROOT_DIR,\n",
    "            transforms=[\n",
    "                CropSample(SAMPLE_LENGTH),\n",
    "                ToTensor(),\n",
    "            ],\n",
    "        )\n",
    "        test_dataset = TransitionDataset(\n",
    "            test_df,\n",
    "            root_dir,\n",
    "            transforms=[\n",
    "                CropSample(sample_length),\n",
    "                ToTensor(),\n",
    "            ],\n",
    "        )\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        opt = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "        loss_fn = nn.BCEWithLogitsLoss(\n",
    "            pos_weight=tensor(train_dataset.get_pos_weight()),\n",
    "        )\n",
    "        val_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        train_metrics = Metrics(\"train\", len(train_dataset), DEVICE)\n",
    "        val_metrics = Metrics(\"val\", len(val_dataset), DEVICE)\n",
    "        test_metrics = Metrics(\"test\", len(test_dataset), DEVICE)\n",
    "\n",
    "        # Train model\n",
    "        train_metrics, val_metrics, best_model = fit(\n",
    "            model,\n",
    "            opt,\n",
    "            loss_fn,\n",
    "            val_loss_fn,\n",
    "            train_metrics,\n",
    "            val_metrics,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=100,\n",
    "        )\n",
    "\n",
    "        running_loss += min(val_metrics.saved_metrics[\"loss\"])\n",
    "        running_bac += max(val_metrics.saved_metrics[\"bac\"])\n",
    "        running_f1 += max(val_metrics.saved_metrics[\"f1\"])\n",
    "\n",
    "        model.load_state_dict(best_model)\n",
    "        # Test the best model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in track(test_loader, description=\"Testing\"):\n",
    "                sample, label = data[\"signal\"].to(DEVICE), data[\"label\"].to(DEVICE)\n",
    "                logits = model(sample)\n",
    "\n",
    "                loss = val_loss_fn(logits, label.float())\n",
    "\n",
    "                predictions = F.sigmoid(logits)\n",
    "                test_metrics.update(predictions, label, loss)\n",
    "        test_metrics.save_metrics(0)\n",
    "\n",
    "    return running_f1 / splits, running_bac / splits, running_loss / splits\n",
    "\n",
    "\n",
    "storage = optuna.storages.RDBStorage(\n",
    "    f\"sqlite:///{RESULTS_DIR}/optuna.db\",\n",
    "    heartbeat_interval=10,\n",
    "    failed_trial_callback=optuna.storages.RetryFailedTrialCallback(),\n",
    ")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    storage=storage,\n",
    "    study_name=STUDY_NAME,\n",
    "    directions=[\"maximize\", \"maximize\", \"minimize\"],\n",
    "    load_if_exists=True,\n",
    ")\n",
    "study.set_metric_names([\"f1\", \"bac\", \"loss\"])\n",
    "study.optimize(\n",
    "    objective,\n",
    "    n_trials=10,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
